{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Woof vs. Meow - 3\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "In this part, we shall use pretrained network VGG-16 to extract image features from the cats and dog images. Then we will use a simple Multilayer perceptron to classify the images using the above extracted features as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data  \n",
    "All you need is the train set  \n",
    "The recommended folder structure is:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data/\n",
    "    train/\n",
    "        dogs/ ### 1024 pictures\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ ### 1024 pictures\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/ ### 416 pictures\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ ### 416 pictures\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pillow in /Library/Python/2.7/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##This notebook is built around using tensorflow as the backend for keras\n",
    "!pip install pillow\n",
    "!KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "import scipy\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training a convolutionnal neural network can be very time-consuming and require a lot of datas.  \n",
    "\n",
    "We can go beyond the previous models in terms of performance and efficiency by using a general-purpose, pre-trained image classifier.  This example uses VGG16, a model trained on the ImageNet dataset - which contains millions of images classified in 1000 categories. \n",
    "\n",
    "On top of it, we add a small multi-layer perceptron and we train it on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Network\n",
    "\n",
    "The VGG-16 is a 16-layer network used by the VGG team in the ILSVRC-2014 competition. The network architecture and implementation details can be found in this [paper](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "In the paper, the VGG-16 model is denoted as configuration D. It achieves 7.5% top-5 error on ILSVRC-2012-val, 7.4% top-5 error on ILSVRC-2012-test.\n",
    "\n",
    "Though the VGG-16 model did not win the ILSVRC competition, the simplicity of the sequential model has led to it being used for a lot of transfer learning tasks.\n",
    "\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "Andrew Ng, chief scientist at Baidu and professor at Stanford, said during his widely popular NIPS 2016 tutorial that transfer learning will be -- after supervised learning -- the next driver of ML commercial success.\n",
    "\n",
    "The task of a Supervised learning problem is to train a model for some task and domain, assuming that we are provided with labeled data for the same task and domain. However, the traditional supervised learning paradigm breaks down when we do not have sufficient labeled data for the task or domain we care about to train a reliable model. In such cases, we could apply a model that has been trained on a similar domain, but practically the model performance deteriorates as the model has inherited the bias of its training data and does not know how to generalize to the new domain, as we have previously explored in [No Free Lunch Theorem](https://github.com/darshanbagul/USPS_Digit_Classification/blob/master/USPS_Digit_Classification.ipynb).\n",
    "\n",
    "Transfer learning allows us to deal with these scenarios by leveraging the already existing labeled data of some related task or domain. We try to store this knowledge gained in solving the source task in the source domain and apply it to our problem of interest. This knowledge can take on various forms depending on the data: it can pertain to how objects are composed to allow us to more easily identify novel objects; it can be with regard to the general words people use to express their opinions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 + small MLP\n",
    "![VGG16 + Dense layers Schema](images/vgg-16-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16 model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height,3)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading VGG16 weights\n",
    "This part is a bit complicated because the structure of our model is not exactly the same as the one used when training weights.  \n",
    "Otherwise, we would use the `model.load_weights()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note : the VGG16 weights file (~500MB) is not included in this repository. You can download from here :  \n",
    "https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('models/vgg/vgg16_weights.h5')\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model_vgg.layers) - 1:\n",
    "        # we don't look at the last two layers in the savefile (fully-connected and activation)\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    layer = model_vgg.layers[k]\n",
    "\n",
    "    if layer.__class__.__name__ in ['Convolution1D', 'Convolution2D', 'Convolution3D', 'AtrousConvolution2D']:\n",
    "        weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the VGG16 model to process samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20002 images belonging to 2 classes.\n",
      "Found 4998 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator_bottleneck = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "validation_generator_bottleneck = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a long process, so we save the output of the VGG16 once and for all.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bottleneck_features_train = model_vgg.predict_generator(train_generator_bottleneck, nb_train_samples)\n",
    "np.save(open('models/bottleneck_features_train.npy', 'wb'), bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bottleneck_features_validation = model_vgg.predict_generator(validation_generator_bottleneck, nb_validation_samples)\n",
    "np.save(open('models/bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.load(open('models/bottleneck_features_train.npy', 'rb'))\n",
    "train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "validation_data = np.load(open('models/bottleneck_features_validation.npy', 'rb'))\n",
    "validation_labels = np.array([0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define and train the custom fully connected neural network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_top = Sequential()\n",
    "model_top.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model_top.add(Dense(256, activation='relu'))\n",
    "model_top.add(Dropout(0.5))\n",
    "model_top.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_top.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20002 samples, validate on 4998 samples\n",
      "Epoch 1/40\n",
      "20002/20002 [==============================] - 34s - loss: 0.4605 - acc: 0.8115 - val_loss: 0.3205 - val_acc: 0.8653\n",
      "Epoch 2/40\n",
      "20002/20002 [==============================] - 35s - loss: 0.3271 - acc: 0.8635 - val_loss: 0.3182 - val_acc: 0.8703\n",
      "Epoch 3/40\n",
      "20002/20002 [==============================] - 38s - loss: 0.2985 - acc: 0.8758 - val_loss: 0.3651 - val_acc: 0.8597\n",
      "Epoch 4/40\n",
      "20002/20002 [==============================] - 34s - loss: 0.2855 - acc: 0.8820 - val_loss: 0.3076 - val_acc: 0.8723\n",
      "Epoch 5/40\n",
      "20002/20002 [==============================] - 35s - loss: 0.2727 - acc: 0.8888 - val_loss: 0.3095 - val_acc: 0.8747\n",
      "Epoch 6/40\n",
      "20002/20002 [==============================] - 34s - loss: 0.2609 - acc: 0.8927 - val_loss: 0.3091 - val_acc: 0.8814\n",
      "Epoch 7/40\n",
      "20002/20002 [==============================] - 35s - loss: 0.2546 - acc: 0.8964 - val_loss: 0.3124 - val_acc: 0.8794\n",
      "Epoch 8/40\n",
      "20002/20002 [==============================] - 35s - loss: 0.2479 - acc: 0.9033 - val_loss: 0.3751 - val_acc: 0.8623\n",
      "Epoch 9/40\n",
      "20002/20002 [==============================] - 38s - loss: 0.2427 - acc: 0.9045 - val_loss: 0.3136 - val_acc: 0.8828\n",
      "Epoch 10/40\n",
      "20002/20002 [==============================] - 33s - loss: 0.2339 - acc: 0.9077 - val_loss: 0.3744 - val_acc: 0.8691\n",
      "Epoch 11/40\n",
      "20002/20002 [==============================] - 33s - loss: 0.2304 - acc: 0.9108 - val_loss: 0.3700 - val_acc: 0.8709\n",
      "Epoch 12/40\n",
      "20002/20002 [==============================] - 33s - loss: 0.2321 - acc: 0.9125 - val_loss: 0.3365 - val_acc: 0.8818\n",
      "Epoch 13/40\n",
      "20002/20002 [==============================] - 36s - loss: 0.2170 - acc: 0.9184 - val_loss: 0.3576 - val_acc: 0.8818\n",
      "Epoch 14/40\n",
      "20002/20002 [==============================] - 36s - loss: 0.2161 - acc: 0.9183 - val_loss: 0.4102 - val_acc: 0.8577\n",
      "Epoch 15/40\n",
      "20002/20002 [==============================] - 36s - loss: 0.2090 - acc: 0.9238 - val_loss: 0.3768 - val_acc: 0.8760\n",
      "Epoch 16/40\n",
      "20002/20002 [==============================] - 36s - loss: 0.2081 - acc: 0.9234 - val_loss: 0.3786 - val_acc: 0.8788\n",
      "Epoch 17/40\n",
      "20002/20002 [==============================] - 36s - loss: 0.2024 - acc: 0.9276 - val_loss: 0.4181 - val_acc: 0.8760\n",
      "Epoch 18/40\n",
      "20002/20002 [==============================] - 36s - loss: 0.1972 - acc: 0.9264 - val_loss: 0.3963 - val_acc: 0.8842\n",
      "Epoch 19/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.2000 - acc: 0.9278 - val_loss: 0.4010 - val_acc: 0.8844\n",
      "Epoch 20/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1936 - acc: 0.9302 - val_loss: 0.4181 - val_acc: 0.8786\n",
      "Epoch 21/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1871 - acc: 0.9322 - val_loss: 0.4237 - val_acc: 0.8727\n",
      "Epoch 22/40\n",
      "20002/20002 [==============================] - 38s - loss: 0.1863 - acc: 0.9334 - val_loss: 0.4228 - val_acc: 0.8786\n",
      "Epoch 23/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1786 - acc: 0.9365 - val_loss: 0.4557 - val_acc: 0.8802\n",
      "Epoch 24/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1785 - acc: 0.9387 - val_loss: 0.4686 - val_acc: 0.8814\n",
      "Epoch 25/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1752 - acc: 0.9383 - val_loss: 0.4648 - val_acc: 0.8802\n",
      "Epoch 26/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1724 - acc: 0.9425 - val_loss: 0.4586 - val_acc: 0.8766\n",
      "Epoch 27/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1657 - acc: 0.9429 - val_loss: 0.4823 - val_acc: 0.8790\n",
      "Epoch 28/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1692 - acc: 0.9426 - val_loss: 0.4981 - val_acc: 0.8794\n",
      "Epoch 29/40\n",
      "20002/20002 [==============================] - 38s - loss: 0.1673 - acc: 0.9454 - val_loss: 0.4996 - val_acc: 0.8844\n",
      "Epoch 30/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1597 - acc: 0.9462 - val_loss: 0.4926 - val_acc: 0.8731\n",
      "Epoch 31/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1624 - acc: 0.9455 - val_loss: 0.5109 - val_acc: 0.8810\n",
      "Epoch 32/40\n",
      "20002/20002 [==============================] - 39s - loss: 0.1598 - acc: 0.9485 - val_loss: 0.5868 - val_acc: 0.8583\n",
      "Epoch 33/40\n",
      "20002/20002 [==============================] - 38s - loss: 0.1661 - acc: 0.9483 - val_loss: 0.5382 - val_acc: 0.8762\n",
      "Epoch 34/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1577 - acc: 0.9489 - val_loss: 0.5348 - val_acc: 0.8794\n",
      "Epoch 35/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1551 - acc: 0.9505 - val_loss: 0.5767 - val_acc: 0.8784\n",
      "Epoch 36/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1553 - acc: 0.9509 - val_loss: 0.5760 - val_acc: 0.8647\n",
      "Epoch 37/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1497 - acc: 0.9540 - val_loss: 0.5773 - val_acc: 0.8776\n",
      "Epoch 38/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1563 - acc: 0.9518 - val_loss: 0.5505 - val_acc: 0.8754\n",
      "Epoch 39/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1440 - acc: 0.9544 - val_loss: 0.5515 - val_acc: 0.8719\n",
      "Epoch 40/40\n",
      "20002/20002 [==============================] - 37s - loss: 0.1420 - acc: 0.9552 - val_loss: 0.6482 - val_acc: 0.8764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10fbc2190>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epoch=40\n",
    "model_top.fit(train_data, train_labels,\n",
    "          nb_epoch=nb_epoch, batch_size=32,\n",
    "          validation_data=(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process of this small neural network is very fast : ~37s per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_top.save_weights('models/bottleneck_40_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_top.load_weights('models/bottleneck_40_epochs.h5')\n",
    "#model_top.load_weights('/notebook/Data1/Code/keras-workshop/models/with-bottleneck/1000-samples--100-epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992/4998 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.64820161735972148, 0.87635054021608638]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.evaluate(validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We reached a 87% accuracy on the validation after 40 epochs, which is a massive boost on our baseline model!**\n",
    "\n",
    "Next, we shall try and finetune the model so that it adapts on this dataset, in order to further boost the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
