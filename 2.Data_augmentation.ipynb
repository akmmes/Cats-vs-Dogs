{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Woof vs. Meow - 2\n",
    "\n",
    "### Building an image classification model using CNN along with data augmentation techniques  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data  \n",
    "All you need is the train set  \n",
    "The recommended folder structure is:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data/\n",
    "    train/\n",
    "        dogs/ \n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ \n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/ \n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ \n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pillow in /Library/Python/2.7/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##This notebook is built around using tensorflow as the backend for keras\n",
    "!pip install pillow\n",
    "!KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "import scipy\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20002 images belonging to 2 classes.\n",
      "Found 4998 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 30\n",
    "nb_train_samples = 20002\n",
    "nb_validation_samples = 4998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation for improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying random transformation to our train set, we artificially enhance our dataset with new unseen images.  \n",
    "This will hopefully reduce overfitting and allows better generalization capability for our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of data augmentation applied to a picture:\n",
    "![Example of data augmentation applied to a picture](images/data_augmentation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20002 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen_augmented = ImageDataGenerator(\n",
    "        rescale=1./255,        # normalize pixel values to [0,1]\n",
    "        shear_range=0.2,       # randomly applies shearing transformation\n",
    "        zoom_range=0.2,        # randomly applies shearing transformation\n",
    "        horizontal_flip=True)  # randomly flip the images\n",
    "\n",
    "# same code as before\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20002/20002 [==============================] - 1742s - loss: 0.4735 - acc: 0.8021 - val_loss: 0.4166 - val_acc: 0.8211\n",
      "Epoch 2/30\n",
      "20002/20002 [==============================] - 2018s - loss: 0.4578 - acc: 0.8044 - val_loss: 0.4209 - val_acc: 0.8237\n",
      "Epoch 3/30\n",
      "20002/20002 [==============================] - 2448s - loss: 0.4553 - acc: 0.8051 - val_loss: 0.4589 - val_acc: 0.7985\n",
      "Epoch 4/30\n",
      "20002/20002 [==============================] - 2455s - loss: 0.4615 - acc: 0.8064 - val_loss: 1.2699 - val_acc: 0.6579\n",
      "Epoch 5/30\n",
      "20002/20002 [==============================] - 3132s - loss: 0.4581 - acc: 0.8113 - val_loss: 1.0254 - val_acc: 0.7167\n",
      "Epoch 6/30\n",
      "20002/20002 [==============================] - 2691s - loss: 0.4513 - acc: 0.8062 - val_loss: 0.4289 - val_acc: 0.8139\n",
      "Epoch 7/30\n",
      "20002/20002 [==============================] - 3468s - loss: 0.4471 - acc: 0.8121 - val_loss: 0.4773 - val_acc: 0.7937\n",
      "Epoch 8/30\n",
      "20002/20002 [==============================] - 2280s - loss: 0.4525 - acc: 0.8110 - val_loss: 0.5529 - val_acc: 0.7901\n",
      "Epoch 9/30\n",
      "20002/20002 [==============================] - 1837s - loss: 0.4454 - acc: 0.8121 - val_loss: 0.4230 - val_acc: 0.7983\n",
      "Epoch 10/30\n",
      "20002/20002 [==============================] - 10533s - loss: 0.4477 - acc: 0.8084 - val_loss: 0.9480 - val_acc: 0.7341\n",
      "Epoch 11/30\n",
      "20002/20002 [==============================] - 3945s - loss: 0.4546 - acc: 0.8132 - val_loss: 0.7399 - val_acc: 0.7977\n",
      "Epoch 12/30\n",
      "20002/20002 [==============================] - 2456s - loss: 0.4511 - acc: 0.8073 - val_loss: 0.4307 - val_acc: 0.8121\n",
      "Epoch 13/30\n",
      "20002/20002 [==============================] - 4667s - loss: 0.4504 - acc: 0.8080 - val_loss: 0.4793 - val_acc: 0.8303\n",
      "Epoch 14/30\n",
      "20002/20002 [==============================] - 3005s - loss: 0.4611 - acc: 0.8114 - val_loss: 0.4080 - val_acc: 0.8183\n",
      "Epoch 15/30\n",
      "20002/20002 [==============================] - 1035s - loss: 0.4430 - acc: 0.8125 - val_loss: 0.5078 - val_acc: 0.8307\n",
      "Epoch 16/30\n",
      "20002/20002 [==============================] - 1384s - loss: 0.4496 - acc: 0.8036 - val_loss: 0.3993 - val_acc: 0.8279\n",
      "Epoch 17/30\n",
      "20002/20002 [==============================] - 1046s - loss: 0.4584 - acc: 0.8063 - val_loss: 0.4006 - val_acc: 0.8127\n",
      "Epoch 18/30\n",
      "20002/20002 [==============================] - 1042s - loss: 0.4609 - acc: 0.8110 - val_loss: 0.7515 - val_acc: 0.7283\n",
      "Epoch 19/30\n",
      "20002/20002 [==============================] - 1053s - loss: 0.4614 - acc: 0.8038 - val_loss: 0.6102 - val_acc: 0.7011\n",
      "Epoch 20/30\n",
      "20002/20002 [==============================] - 1024s - loss: 0.4653 - acc: 0.8043 - val_loss: 0.5565 - val_acc: 0.8067\n",
      "Epoch 21/30\n",
      "20002/20002 [==============================] - 1029s - loss: 0.4543 - acc: 0.8067 - val_loss: 0.5105 - val_acc: 0.7735\n",
      "Epoch 22/30\n",
      "20002/20002 [==============================] - 1063s - loss: 0.4608 - acc: 0.8003 - val_loss: 0.5347 - val_acc: 0.8041\n",
      "Epoch 23/30\n",
      "20002/20002 [==============================] - 2519s - loss: 0.4838 - acc: 0.8050 - val_loss: 0.4816 - val_acc: 0.8091\n",
      "Epoch 24/30\n",
      "20002/20002 [==============================] - 1037s - loss: 0.4593 - acc: 0.8090 - val_loss: 0.4928 - val_acc: 0.7787\n",
      "Epoch 25/30\n",
      "20002/20002 [==============================] - 1014s - loss: 0.4645 - acc: 0.8058 - val_loss: 0.4778 - val_acc: 0.7617\n",
      "Epoch 26/30\n",
      "20002/20002 [==============================] - 1050s - loss: 0.4687 - acc: 0.8055 - val_loss: 0.7349 - val_acc: 0.6931\n",
      "Epoch 27/30\n",
      "20002/20002 [==============================] - 5062s - loss: 0.4666 - acc: 0.8017 - val_loss: 0.7341 - val_acc: 0.7881\n",
      "Epoch 28/30\n",
      "20002/20002 [==============================] - 1127s - loss: 0.4821 - acc: 0.7999 - val_loss: 0.4409 - val_acc: 0.7977\n",
      "Epoch 29/30\n",
      "20002/20002 [==============================] - 1117s - loss: 0.4703 - acc: 0.8011 - val_loss: 0.3945 - val_acc: 0.8339\n",
      "Epoch 30/30\n",
      "20002/20002 [==============================] - 1092s - loss: 0.4668 - acc: 0.8019 - val_loss: 0.5384 - val_acc: 0.7767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10f3c11d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator_augmented,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/augmented_30_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/augmented_30_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.53702074628011753, 0.77991196480976577]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to data augmentation, the accuracy of the model decreased slightly to 78% ** \n",
    "\n",
    "This is because our model has now become more robust to scale and rotation changes, and generalizes better on unseen data at the expense of accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
