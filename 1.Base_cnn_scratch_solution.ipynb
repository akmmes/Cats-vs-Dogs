{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Woof vs. Meow - 1\n",
    "\n",
    "### Building an image classification model using a CNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data  \n",
    "All you need is the train set  \n",
    "The recommended folder structure is:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data/\n",
    "    train/\n",
    "        dogs/ ### 1024 pictures\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ ### 1024 pictures\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/ ### 416 pictures\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ ### 416 pictures\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "Note : for this example we only consider 2x1000 training images and 2x400 testing images among the 2x12500 available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The github repo includes about 1500 images for this model. The original Kaggle dataset is much larger. The purpose of this demo is to show how you can build models with smaller size datasets. You should be able to improve this model by using more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pillow in /Library/Python/2.7/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##This notebook is built around using tensorflow as the backend for keras\n",
    "!pip install pillow\n",
    "!KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "import scipy\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20002 images belonging to 2 classes.\n",
      "Found 4998 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Conv Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 30\n",
    "nb_train_samples = 20002\n",
    "nb_validation_samples = 4998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20002/20002 [==============================] - 1656s - loss: 0.5280 - acc: 0.7481 - val_loss: 0.4870 - val_acc: 0.7711\n",
      "Epoch 2/30\n",
      "20002/20002 [==============================] - 4211s - loss: 0.4983 - acc: 0.7725 - val_loss: 0.4711 - val_acc: 0.7733\n",
      "Epoch 3/30\n",
      "20002/20002 [==============================] - 1741s - loss: 0.4717 - acc: 0.7871 - val_loss: 0.4466 - val_acc: 0.8021\n",
      "Epoch 4/30\n",
      "20002/20002 [==============================] - 1479s - loss: 0.4610 - acc: 0.7934 - val_loss: 0.4130 - val_acc: 0.8113\n",
      "Epoch 5/30\n",
      "20002/20002 [==============================] - 1676s - loss: 0.4383 - acc: 0.8066 - val_loss: 0.4496 - val_acc: 0.8005\n",
      "Epoch 6/30\n",
      "20002/20002 [==============================] - 1613s - loss: 0.4253 - acc: 0.8198 - val_loss: 0.4187 - val_acc: 0.8121\n",
      "Epoch 7/30\n",
      "20002/20002 [==============================] - 1518s - loss: 0.4110 - acc: 0.8230 - val_loss: 0.4552 - val_acc: 0.8039\n",
      "Epoch 8/30\n",
      "20002/20002 [==============================] - 1537s - loss: 0.4112 - acc: 0.8262 - val_loss: 0.4362 - val_acc: 0.8027\n",
      "Epoch 9/30\n",
      "20002/20002 [==============================] - 1567s - loss: 0.4088 - acc: 0.8276 - val_loss: 0.4316 - val_acc: 0.8043\n",
      "Epoch 10/30\n",
      "20002/20002 [==============================] - 1634s - loss: 0.4057 - acc: 0.8286 - val_loss: 0.4772 - val_acc: 0.7955\n",
      "Epoch 11/30\n",
      "20002/20002 [==============================] - 1562s - loss: 0.3983 - acc: 0.8336 - val_loss: 0.3932 - val_acc: 0.8243\n",
      "Epoch 12/30\n",
      "20002/20002 [==============================] - 2121s - loss: 0.4014 - acc: 0.8355 - val_loss: 0.4151 - val_acc: 0.8291\n",
      "Epoch 13/30\n",
      "20002/20002 [==============================] - 1731s - loss: 0.4026 - acc: 0.8362 - val_loss: 0.4180 - val_acc: 0.8199\n",
      "Epoch 14/30\n",
      "20002/20002 [==============================] - 23961s - loss: 0.4035 - acc: 0.8365 - val_loss: 0.4018 - val_acc: 0.8233\n",
      "Epoch 15/30\n",
      "20002/20002 [==============================] - 23097s - loss: 0.3964 - acc: 0.8349 - val_loss: 0.3987 - val_acc: 0.8203\n",
      "Epoch 16/30\n",
      "20002/20002 [==============================] - 1470s - loss: 0.4006 - acc: 0.8395 - val_loss: 0.3801 - val_acc: 0.8347\n",
      "Epoch 17/30\n",
      "20002/20002 [==============================] - 3754s - loss: 0.4052 - acc: 0.8383 - val_loss: 0.4013 - val_acc: 0.8235\n",
      "Epoch 18/30\n",
      "20002/20002 [==============================] - 2495s - loss: 0.4020 - acc: 0.8387 - val_loss: 0.4076 - val_acc: 0.8169\n",
      "Epoch 19/30\n",
      "20002/20002 [==============================] - 3191s - loss: 0.4008 - acc: 0.8406 - val_loss: 0.4603 - val_acc: 0.8153\n",
      "Epoch 20/30\n",
      "20002/20002 [==============================] - 1616s - loss: 0.3998 - acc: 0.8482 - val_loss: 1.3175 - val_acc: 0.7481\n",
      "Epoch 21/30\n",
      "20002/20002 [==============================] - 1503s - loss: 0.4060 - acc: 0.8399 - val_loss: 0.4207 - val_acc: 0.8187\n",
      "Epoch 22/30\n",
      "20002/20002 [==============================] - 1493s - loss: 0.4170 - acc: 0.8375 - val_loss: 0.5455 - val_acc: 0.7787\n",
      "Epoch 23/30\n",
      "20002/20002 [==============================] - 1571s - loss: 0.4084 - acc: 0.8389 - val_loss: 0.5054 - val_acc: 0.8039\n",
      "Epoch 24/30\n",
      "20002/20002 [==============================] - 1761s - loss: 0.4153 - acc: 0.8386 - val_loss: 0.4699 - val_acc: 0.8057\n",
      "Epoch 25/30\n",
      "20002/20002 [==============================] - 5408s - loss: 0.4207 - acc: 0.8389 - val_loss: 0.4077 - val_acc: 0.8281\n",
      "Epoch 26/30\n",
      "20002/20002 [==============================] - 1559s - loss: 0.4412 - acc: 0.8344 - val_loss: 0.4487 - val_acc: 0.8365\n",
      "Epoch 27/30\n",
      "20002/20002 [==============================] - 1589s - loss: 0.4282 - acc: 0.8294 - val_loss: 0.5008 - val_acc: 0.7925\n",
      "Epoch 28/30\n",
      "20002/20002 [==============================] - 1599s - loss: 0.4292 - acc: 0.8357 - val_loss: 0.4308 - val_acc: 0.8167\n",
      "Epoch 29/30\n",
      "20002/20002 [==============================] - 3623s - loss: 0.4544 - acc: 0.8277 - val_loss: 0.4377 - val_acc: 0.8117\n",
      "Epoch 30/30\n",
      "20002/20002 [==============================] - 1642s - loss: 0.4363 - acc: 0.8293 - val_loss: 0.4842 - val_acc: 0.8151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1110e4c50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/basic_cnn_30_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/basic_cnn_30_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model successfully runs at one epoch, go back and it for 30 epochs by changing nb_epoch above.  I was able to get to an val_acc of **0.82** in 30 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47102052572012998, 0.82032813122864956]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After 30 epochs the convolutional neural network reaches 82% accuracy. We can witness overfitting, no progress is made over validation set in the next epochs**\n",
    "\n",
    "This shall be our baseline accuracy, on which we will try and improve in the further models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
